{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Construcción de una Red Neuronal desde cero\n",
    "A través de este tutorial, se va a construir una Red Neuronal Artificial (RNA) explicando de forma teórica los diferentes elementos necesarios para la construcción y entrenamiento del modelo. Además, se pondrá en práctica la red para evaluar su correcto funcionamiento."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Índice de contenidos\n",
    "- [1 - Inicialización de capas](#1)\n",
    "- [2 - Inicialización de parámetros](#2)\n",
    "- [3 - Implementar \"forward propagation\"](#3)\n",
    "- [4 - Computación del coste](#4)\n",
    "- [5 - Implementación del \"backward propagation\"](#5)\n",
    "- [6 - Actualización de parámetros (gradient descent)](#6)\n",
    "- [7 - Entrenamiento del modelo](#7)\n",
    "- [8 - Predicción](#8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.- Instalación de librerías\n",
    "En primer lugar, se instalan e importan todas las librerías necesarias para la realización de la implementación.\n",
    "\n",
    "## 1.1.- Instalación de las librerías\n",
    "Se va a emplear 'numpy' y 'matplotlib':"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (1.26.4)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable.It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.1.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Collecting matplotlib\n",
      "  Downloading matplotlib-3.9.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)\n",
      "Collecting contourpy>=1.0.1 (from matplotlib)\n",
      "  Downloading contourpy-1.2.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.8 kB)\n",
      "Collecting cycler>=0.10 (from matplotlib)\n",
      "  Downloading cycler-0.12.1-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting fonttools>=4.22.0 (from matplotlib)\n",
      "  Downloading fonttools-4.53.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (162 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m162.6/162.6 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting kiwisolver>=1.3.1 (from matplotlib)\n",
      "  Downloading kiwisolver-1.4.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.4 kB)\n",
      "Requirement already satisfied: numpy>=1.23 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (24.1)\n",
      "Collecting pillow>=8 (from matplotlib)\n",
      "  Downloading pillow-10.4.0-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (9.2 kB)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /usr/lib/python3/dist-packages (from matplotlib) (2.4.7)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /root/.local/lib/python3.11/site-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
      "Downloading matplotlib-3.9.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (8.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.3/8.3 MB\u001b[0m \u001b[31m34.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading contourpy-1.2.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (306 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m306.0/306.0 kB\u001b[0m \u001b[31m28.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading cycler-0.12.1-py3-none-any.whl (8.3 kB)\n",
      "Downloading fonttools-4.53.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.9/4.9 MB\u001b[0m \u001b[31m40.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading kiwisolver-1.4.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m36.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading pillow-10.4.0-cp311-cp311-manylinux_2_28_x86_64.whl (4.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.5/4.5 MB\u001b[0m \u001b[31m38.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: pillow, kiwisolver, fonttools, cycler, contourpy, matplotlib\n",
      "Successfully installed contourpy-1.2.1 cycler-0.12.1 fonttools-4.53.1 kiwisolver-1.4.5 matplotlib-3.9.1 pillow-10.4.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable.It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.1.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install numpy\n",
    "%pip install matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-03 15:58:56.557509: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-08-03 15:58:56.632749: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-08-03 15:58:56.653853: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-08-03 15:58:56.793643: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1722700739.267844    1655 cuda_executor.cc:1001] could not open file to read NUMA node: /sys/bus/pci/devices/0000:07:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "I0000 00:00:1722700739.321892    1655 cuda_executor.cc:1001] could not open file to read NUMA node: /sys/bus/pci/devices/0000:07:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "I0000 00:00:1722700739.321937    1655 cuda_executor.cc:1001] could not open file to read NUMA node: /sys/bus/pci/devices/0000:07:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2.- Inicialización de librerías\n",
    "Una vez instaladas, se inicializan las librerías:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.- Inicialización de capas\n",
    "Este método nos permite definir la estructura de la red neuronal que vamos a construir. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_layers(X, Y, num_hidden_layers, num_hidden_units):\n",
    "    \"\"\"\n",
    "    Este método permite definir la estructura de la red neuronal, especificando el número de neuronas en la capa de entrada (que viene dada por el tamaño del dataset utilizado),\n",
    "    el número de capas ocultas especificado por parámetro de entrada, el número de neuronas en cada una de estas capas ocultas y finalmente, el número de neuronas en la capa de\n",
    "    salida (especificado como el tamaño de la variable objetivo).\n",
    "\n",
    "    Se entiende por tanto que se está construyendo una Red Neuronal Profunda donde tocas las capas ocultas tienen el mismo número de neuronas.\n",
    "\n",
    "    Variables de entrada:\n",
    "    X -- conjunto de datos de entrada (tamaño de entrada, número de ejemplos)\n",
    "    Y -- etiquetas (tamaño de salida, número de ejemplos)\n",
    "    \n",
    "    Variables de salida:\n",
    "    input_units -- número de neuronas en la capa de entrada (equivalente al número de variables de entrada)\n",
    "    hidden_units -- número de neuronas en las capas ocultas\n",
    "    output_units -- número de neuronas en la capa de salida\n",
    "    \"\"\"\n",
    "    \n",
    "    input_units = X.shape[0]\n",
    "    hidden_layers = num_hidden_layers\n",
    "    hidden_units = num_hidden_units \n",
    "    output_units = Y.shape[0]\n",
    "\n",
    "    return (input_units, hidden_layers, hidden_units, output_units)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.- Inicialización de parámetros\n",
    "Este método nos permite inicializar los parámetros para una red neuronal de `l` capas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_parameters(dims_layers):\n",
    "    \"\"\"\n",
    "    Esta función nos permitirá inicializar los parámetros de la Red Neuronal con la estructura definida en la función anterior.\n",
    "\n",
    "    Variables de entrada:\n",
    "    dims_layers -- consiste en las dimensiones de la red.\n",
    "\n",
    "    Variables de salida\n",
    "    parameters -- consiste en el conjunto de parámetros inicializados en la red\n",
    "    \"\"\"\n",
    "\n",
    "    # Se crea el diccionario para almacenar los parámetros\n",
    "    paramaters = {}\n",
    "\n",
    "    for num_capa in range(1, dims_layers[hidden_layers] + 2):\n",
    "        # Para cada capa se debe de crear la matriz de pesos y el sesgo correspondiente.\n",
    "        paramaters['W' + str(num_capa)] = np.random.rand(dims_layers[hidden_units], dims_layers[hidden_units - 1])\n",
    "        paramaters['b' + str(num_capa)] = np.random.rand(dims_layers[hidden_units], 1)\n",
    "\n",
    "    # Devuelvo los parámetros inicializados\n",
    "    return paramaters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.- Implementación de 'forward propagation'\n",
    "Para implementar la propagación hacia delante, se deben de realizar los siguientes cálculos:\n",
    "* Cálculo de los parámetros pre-activación.\n",
    "* Cálculo de la activación de las neuronas.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1.- Parámetros pre-activación\n",
    "Este método se encarga de calcular los parámetros que se introducen en la función de activación de la neurona. La ecuación que se emplea es:\n",
    "$$Z^{[l]} = W^{[l]}A^{[l-1]} +b^{[l]}\\tag{4}$$\n",
    "donde ${[l]}$ es el número de capa; $W^{[l]}$ son los pesos actuales de la capa actual; $A^{[l-1]}$ es la activación de la capa anterior; $b^{[l]}$ es el bias de la capa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_activation(A, W, b):\n",
    "    \"\"\"_summary_\n",
    "    Esta función permite calcular los parámetros pre-activacion de la neurona.\n",
    "    \n",
    "    Args:\n",
    "        A (_type_): consiste en las activaciones obtenidas en la capa anterior.\n",
    "        W (_type_): consiste en los pesos actuales de la capa.\n",
    "        b (_type_): consiste en el sesgo actual de la capa.\n",
    "    \"\"\"\n",
    "\n",
    "    # Obtengo los parámetros de pre-activación\n",
    "    Z = np.dot(W,A) + b\n",
    "\n",
    "    # Creo una tupla de Python guardada en cache para acelerar el entrenamiento de la red, ya que estos parámetros se usarán más en adelante\n",
    "    pre_activation_params = (A, W, b)\n",
    "\n",
    "    return Z, pre_activation_params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2.- Activación de la capa\n",
    "Este método se encargará de calcular la activación de la neurona ante los parámetros pre-activación. Son múltiples las opciones que podemos emplear para realizar este paso. A continuación, explicamos y definimos las más comunes:\n",
    "* **Sigmoid**: \n",
    "* **ReLU**:\n",
    "* **Softmax**: \n",
    "* **Leaky ReLU**: \n",
    "* **Tanh**: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2.1.- Sigmoid\n",
    "La función sigmoidal sigue la siguiente fórmula matemática:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    activation = 1 / (1 +  np.exp(-z))\n",
    "\n",
    "    return activation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2.2.- ReLU\n",
    "La función ReLU sigue la siguiente fórmula matemática:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(z):\n",
    "    activation = max(0,z)\n",
    "\n",
    "    return activation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2.3.- Softmax\n",
    "La función Softmax sigue la siguiente fórmula matemática:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(z):\n",
    "    max_z = np.max(z)\n",
    "    exp = np.exp(z-max_z)\n",
    "    activation = exp/max_z\n",
    "\n",
    "    return activation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2.4.- Leaky ReLU\n",
    "La función Leaky ReLU sigue la siguiente fórmula matemática:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def leaky_relu(z):\n",
    "    activation = max(0.01*z,z)\n",
    "\n",
    "    return activation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2.5.- Tanh\n",
    "La función Tanh sigue la siguiente fórmula matemática:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tanh(z):\n",
    "    activation = np.tanh(z)\n",
    "\n",
    "    return activation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5.- Computación de la pérdida\n",
    "En este punto, ya tenemos computada la activación de cada capa por lo que tan sólo quedaría computar el coste. El coste define el error existente entre las predicciones del modelo y las etiquetas reales del conjunto de datos. Para asegurarnos de que nuestro modelo está aprendiendo correctamente, el coste debería de reducirse en cada etapa del entrenamiento.\n",
    "\n",
    "Procedemos por tanto a definir algunas de las funciones de pérdida (que son las funciones que calculan el 'coste' sobre un conjunto de datos) más comunes:\n",
    "* **Cross-entropy**: \n",
    "* **Log Loss**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cost_function(predictions, Y):\n",
    "    \"\"\"_summary_\n",
    "    Función que permite calcular el error entre las predicciones del modelo y las etiquetas correctas.\n",
    "\n",
    "    Args:\n",
    "        predictions (_type_): _description_\n",
    "        Y (_type_): _description_\n",
    "    \"\"\"\n",
    "    # Obtengo el número de ejemplos\n",
    "    num_examples = Y.shape[1]\n",
    "\n",
    "    # Calculo la función de pérdida\n",
    "    cost = -(1/num_examples) * np.sum( Y * np.log(predictions) + (1-Y) * np.log(1-predictions))\n",
    "\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6.- Implementación del backward propagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7.- Actualización de parámetros (Gradient Descent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8.- Entrenamiento del modelo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9.- Predicción"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10.- Conclusión"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
