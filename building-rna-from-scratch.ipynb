{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Construcción de una Red Neuronal desde cero\n",
    "A través de este tutorial, se va a construir una Red Neuronal Artificial (RNA) explicando de forma teórica los diferentes elementos necesarios para la construcción y entrenamiento del modelo. Además, se pondrá en práctica la red para evaluar su correcto funcionamiento."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Índice de contenidos\n",
    "- [1 - Inicialización de capas](#1)\n",
    "- [2 - Inicialización de parámetros](#2)\n",
    "- [3 - Implementar \"forward propagation\"](#3)\n",
    "- [4 - Computación del coste](#4)\n",
    "- [5 - Implementación del backward propagation](#5)\n",
    "- [6 - Actualización de parámetros (gradient descent)](#6)\n",
    "- [7 - Entrenamiento del modelo](#7)\n",
    "- [8 - Predicción](#8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.- Instalación de librerías\n",
    "En primer lugar, se instalan e importan todas las librerías necesarias para la realización de la implementación.\n",
    "\n",
    "## 1.1.- Instalación de las librerías\n",
    "Se va a emplear 'numpy' y 'matplotlib':"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install numpy\n",
    "!pip install matplotlib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2.- Inicialización de librerías\n",
    "Una vez instaladas, se inicializan las librerías:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.- Inicialización de capas\n",
    "Este método nos permite definir la estructura de la red neuronal que vamos a construir. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_layers(X, Y):\n",
    "    \"\"\"\n",
    "    Variables de entrada:\n",
    "    X -- conjunto de datos de entrada (tamaño de entrada, número de ejemplos)\n",
    "    Y -- etiquetas (tamaño de salida, número de ejemplos)\n",
    "    \n",
    "    Variables de salida:\n",
    "    input_units -- número de neuronas en la capa de entrada (equivalente al número de variables de entrada)\n",
    "    hidden_units -- númeor de neuronas en la capa oculta\n",
    "    output_units -- número de neuronas en la capa de salida\n",
    "    \"\"\"\n",
    "    \n",
    "    input_units = X.shape[0]\n",
    "    # hidden_units = 2 \n",
    "    output_units = Y.shape[0]\n",
    "\n",
    "    return (input_units, hidden_units, output_units)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.- Inicialización de parámetros\n",
    "Este método nos permite inicializar los parámetros para una red neuronal de `l` capas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.- Implementación de 'forward propagation'\n",
    "Para implementar la propagación hacia delante, se deben de realizar los siguientes cálculos:\n",
    "* Cálculo de los parámetros pre-activación.\n",
    "* Cálculo de la activación de las neuronas.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1.- Parámetros pre-activación\n",
    "Este método se encarga de calcular los parámetros que se introducen en la función de activación de la neurona. La ecuación que se emplea es:\n",
    "$$Z^{[l]} = W^{[l]}A^{[l-1]} +b^{[l]}\\tag{4}$$\n",
    "donde ${[l]}$ es el número de capa; $W^{[l]}$ son los pesos actuales de la capa actual; $A^{[l-1]}$ es la activación de la capa anterior; $b^{[l]}$ es el bias de la capa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculo_preactivacion(A, W, b):\n",
    "    \"\"\"\n",
    "    Implementación del cálculo de parámetros pre-activación de la neurona.\n",
    "\n",
    "    Variables de entrada:\n",
    "    A -- activación de la capa anterior.\n",
    "    W -- matriz de pesos de la capa.\n",
    "    b -- vector de bias.\n",
    "\n",
    "    Variables de salida:\n",
    "    Z -- entrada de la función de activación.\n",
    "    cache -- tupla de Python con la activación de la capa, pesos de la capa y bias. Será empleado más adelante.\n",
    "    \"\"\"\n",
    "    Z = np.dot(W,A)+b\n",
    "    cache = (A, W, b)\n",
    "    \n",
    "    return Z, cache"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2.- Activación de la capa\n",
    "Este método se encargará de calcular la activación de la neurona ante los parámetros pre-activación. Son múltiples las opciones que podemos emplear para realizar este paso. A continuación, explicamos y definimos las más comunes:\n",
    "* **Sigmoid**: \n",
    "* **ReLU**:\n",
    "* **Softmax**: \n",
    "* **Leaky ReLU**: \n",
    "* **Tanh**: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2.1.- Sigmoid\n",
    "La función sigmoidal sigue la siguiente fórmula matemática:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    activation = 1 / (1 +  np.exp(-z))\n",
    "\n",
    "    return activation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2.2.- ReLU\n",
    "La función ReLU sigue la siguiente fórmula matemática:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(z):\n",
    "    activation = max(0,z)\n",
    "\n",
    "    return activation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2.3.- Softmax\n",
    "La función Softmax sigue la siguiente fórmula matemática:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(z):\n",
    "    max_z = np.max(z)\n",
    "    exp = np.exp(z-max_z)\n",
    "    activation = exp/max_z\n",
    "\n",
    "    return activation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2.4.- Leaky ReLU\n",
    "La función Leaky ReLU sigue la siguiente fórmula matemática:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def leaky_relu(z):\n",
    "    activation = max(0.01*z,z)\n",
    "\n",
    "    return activation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2.5.- Tanh\n",
    "La función Tanh sigue la siguiente fórmula matemática:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tanh(z):\n",
    "    activation = np.tanh(z)\n",
    "\n",
    "    return activation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5.- Computación de la pérdida\n",
    "En este punto, ya tenemos computada la activación de cada capa por lo que tan sólo quedaría computar el coste. El coste define el error existente entre las predicciones del modelo y las etiquetas reales del conjunto de datos. Para asegurarnos de que nuestro modelo está aprendiendo correctamente, el coste debería de reducirse en cada etapa del entrenamiento.\n",
    "\n",
    "Procedemos por tanto a definir algunas de las funciones de pérdida (que son las funciones que calculan el 'coste' sobre un conjunto de datos) más comunes:\n",
    "* **Cross-entropy**: \n",
    "* **Log Loss**:"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
